# ðŸ”“ Under the Hood: Unpacking Git's Secrets

**Partâ€¯2: The Pack Index â€” Gitâ€™s Builtâ€‘In Search Engine**

> *Last time we answered â€œWhy build our own Git engine?â€*\
> The takeaway was that every highâ€‘performance scan needs **fast, random access** to Git objects.\
> *Today we build the piece that makes that access O(1): the pack ****index**** (**``**\*\*\*\*\*\*\*\*).*

---

## 0.Â Why an index at all?

Imagine a warehouse that stores every edition of every book ever printed, but all the crates look identical.\
Without an inventory list youâ€™d have to open every crate until you stumble on the right book.

A **Git packfile** is that warehouse: one giant blob with millions of objects jammed together. The **pack index** is the inventory list that tells us **exactly which byte offset** a given object lives at.\
Without it StepÂ 2 (*InflateÂ / Delta resolve*) would degrade from *seekâ€‘andâ€‘read* to *linear scan*,Â a nonâ€‘starter on multiâ€‘gigabyte packs.

---

## 1.Â The Library Card Analogy, Revisited

We keep the cardâ€‘catalog story because itâ€™s intuitive, but add the missing â€œwhyâ€:

| Library metaphor          | What happens in Git                                  | Why it matters                                                          |
| ------------------------- | ---------------------------------------------------- | ----------------------------------------------------------------------- |
| Section guide (Aâ€“F,Â Gâ€“Mâ€¦) | **Fanâ€‘out table** â€” 256 counters, one per firstâ€‘byte | Shrinks our binaryâ€‘search window from *the whole pack* to a tiny slice. |
| Alphabetized rack         | **Sorted hash table**                                | Lets us finish with logâ‚‚(n) comparisons instead of n.                   |
| Bookâ€™s shelf mark         | **Offset array**                                     | Tells us where in the pack to seek.                                     |
| Dustâ€‘jacket barcode       | **CRC32**                                            | Cheap corruption check before we even decompress.                       |

---

## 2.Â Pack Index File Layout (and What Each Field Buys Us)

1. **MagicÂ +Â Version** â€” sanity check; abort early if we mmap the wrong file.
2. **Fanâ€‘out table (256Â Ã—Â 4â€¯bytes)** â€” narrows search window.\
   *Why?* Saves millions of comparisons on large packs.
3. **Sorted SHAâ€‘1 (20â€¯Ã—â€¯N)** â€” final binary search.\
   *Why?* Guarantees log time lookâ€‘ups.
4. **CRCâ€‘32 (4â€¯Ã—â€¯N)** â€” verify bytes *before* we waste CPU inflating them.
5. **32â€‘bit offsets (4â€¯Ã—â€¯N)** â€” jump straight to the object.
6. **Largeâ€‘offset table (optional)** â€” keeps format compact while allowing >â€¯2â€¯GB packs.

> **Why every field matters:** none of these entries are optional. Each one removes a bottleneckâ€”either on *search cost* or on *data integrity*.  either on *search cost* or on *data integrity*.

---

## 3.Â Twoâ€‘Step Search Walkâ€‘through



```go
// idxFile.findObject implements the twoâ€‘stage lookup we described.
func (f *idxFile) findObject(hash Hash) (uint64, bool) {
    first := hash[0]                     // 1ï¸âƒ£ firstâ€‘byte bucket

    // 1. section bounds via fanâ€‘out
    lo := f.fanout[first-1]              // lower inclusive (0 when first==0)
    hi := f.fanout[first] - 1            // upper inclusive

    if lo > hi {                         // empty bucket â†’ object absent
        return 0, false
    }

    // 2. binary search inside that window
    rel, ok := slices.BinarySearchFunc(
        f.oidTable[lo:hi+1], hash,
        func(a, b Hash) int { return bytes.Compare(a[:], b[:]) },
    )
    if !ok {
        return 0, false
    }
    abs := int(lo) + rel
    return f.entries[abs].offset, true
}
```

- **Why firstâ€‘byte bucket?** 256 counters fit in CPU cache.
- **Why binary search next?** Hashes are sorted -> O(logâ€¯k) where *k* is bucket size.

---

## 4.Â Parsing the Index â€” Guided Tour (powered by `parseIdx`)



### 4.1Â Verify header â†’ *Fail fast on wrong file type*

```go
header := make([]byte, 8)
ix.ReadAt(header, 0)
if !bytes.Equal(header[:4], []byte{0xff, 0x74, 0x4f, 0x63}) {
    return nil, fmt.Errorf("not a Git packâ€‘index file")
}
if ver := binary.BigEndian.Uint32(header[4:]); ver != 2 {
    return nil, fmt.Errorf("unsupported idx version %d", ver)
}
```

*Why?* Bail out early if we were handed the wrong file or an ancient index version.

### 4.2Â Read fanâ€‘out â†’ *Learn object count & bucket bounds*

```go
fanoutData := make([]byte, 256*4)
ix.ReadAt(fanoutData, 8) // 8â€‘byte header already consumed
for i := 0; i < 256; i++ {
    f.fanout[i] = binary.BigEndian.Uint32(fanoutData[i*4:])
}
objCount := f.fanout[255] // last entry holds total #objects
```

*Why?* Those 256 counters instantly tell us â€œbucket limitsâ€ and the **total** object count used for slice sizing.

### 4.3Â Slice hashes / CRC / offsets â†’ *Prepare parallel arrays*

```go
// Byte ranges preâ€‘computed from objCount
oidBase   := 8 + 256*4
crcBase   := oidBase   + int64(objCount*20)
offBase   := crcBase  + int64(objCount*4)

// One mmap read for the whole block â†’ fewer syscalls
bulk := make([]byte, objCount*(20+4+4))
ix.ReadAt(bulk, oidBase)

// Unsafe slice trick to avoid perâ€‘hash allocations
oids := *(*[]Hash)(unsafe.Pointer(&bulk[0]))[:objCount:objCount]
crcs := bulk[objCount*20 : objCount*24]
offs := bulk[objCount*24:]
```

*Why?* Reading contiguous blocks and keeping **parallel arrays** keeps lookâ€‘ups cacheâ€‘friendly.

### 4.4Â Handle big packs â†’ *Support monorepos without bloating small ones*

```go
// 32â€‘bit offset with MSB=1 â†’ use 64â€‘bit table
if off32 & 0x8000_0000 != 0 {
    largeIdx := off32 & 0x7fff_ffff
    entries[i].offset = largeOffsets[largeIdx]
} else {
    entries[i].offset = uint64(off32)
}
```

*Why?* Packs smaller than 2â€¯GB stay compact (4â€‘byte offsets); jumbo packs get 64â€‘bit precision only where needed.

---

## 5.Â Putting It to Work

Show a **10â€‘line main** that prints

```
Found 3â€¯123â€¯456 objects across 1 pack
Lookup 45deadâ€¦ â†’ offset 0x1A2B3C, CRC ok
```



---

## 6.Â Performance Snapshot â€” Why the Fanâ€‘out Matters

| Objects | Linear scan | Fanâ€‘out + bin search           | Speedâ€‘up      |
| ------- | ----------- | ------------------------------ | ------------- |
| 10â€¯M    | 10â€¯M comps  | 256 + logâ‚‚(39â€¯000) â‰ˆÂ 272 comps | **\~37â€¯000Ã—** |

Even on SSDs, 37â€¯000 fewer comparisons per lookup is a win you *feel*.

---

## 7.Â Takeaways

- The pack index is not optional; it underpins every O(1) object lookup.
- Fanâ€‘out + sorted table is a textbook example of **twoâ€‘level search**.
- CRCs on compressed bytes give *cheap* corruption checks.
- The format scales from tiny repos to multiâ€‘gigabyte monorepos without waste.

---

### Next: Multiâ€‘Packâ€‘Index (MIDX)

Now that we can find any object *inside one pack*, the next hurdle is: *what if the repo has 20 packs?* A single `.midx` gives us the same O(1) access across them all.\
Weâ€™ll decode that in **PartÂ 3**.

