# ðŸ”“ Under the Hood: Unpacking Git's Secrets

## Part 4: Cracking the Pack â€“ From Raw Bytes to Live Objects

In our journey so far, we've built a powerful indexing system. We can take any object's SHA-1 hash and, using the `.idx` and `.midx` files, instantly find its precise location within a packfile. We have the map and the coordinates.

Today, we arrive at our destination. We'll dive into the packfile itself, decompress its contents, unravel Git's clever delta-encoding scheme, and write the code that turns raw, compressed bytes back into live, usable objects.

---

## ðŸ—œï¸ The First Hurdle: Inflation and Object Headers

When we seek to an object's offset in a packfile, we don't find plain text. We find a small header followed by a stream of bytes compressed with zlib. Our first task is to "inflate" this data.

Before we can decompress, we must read the header to learn two critical facts:

- **Object Type**: Is this a commit, tree, blob, or a delta?
- **Uncompressed Size**: How large will the object be after we inflate it?

### Variable-Length Header Parsing

Git uses a space-saving variable-length encoding for this header:

**Like shipping packages at scale** - a jewelry ring ships in a tiny padded envelope with minimal labeling, while a refrigerator requires a massive crate with extensive handling instructions, weight warnings, and tracking barcodes. The packaging grows precisely to match what's inside. This saves massive space across millions of objects.

```go
// From the actual parseObjectHeaderUnsafe implementation
func parseObjectHeaderUnsafe(data []byte) (ObjectType, uint64, int) {
    if len(data) == 0 {
        return ObjBad, 0, -1
    }

    // First byte contains type and lower bits of size
    b0 := *(*byte)(unsafe.Pointer(&data[0]))
    objType := ObjectType((b0 >> 4) & 7)
    size := uint64(b0 & 0x0f)

    if b0&0x80 == 0 {
        return objType, size, 1
    }

    // Fast path for common 2-3 byte headers
    if len(data) >= 3 {
        b1 := *(*byte)(unsafe.Add(unsafe.Pointer(&data[0]), 1))
        size |= uint64(b1&0x7f) << 4
        if b1&0x80 == 0 {
            return objType, size, 2
        }

        b2 := *(*byte)(unsafe.Add(unsafe.Pointer(&data[0]), 2))
        size |= uint64(b2&0x7f) << 11
        if b2&0x80 == 0 {
            return objType, size, 3
        }
    }

    // Fallback for longer headers
    shift := uint(4)
    for i := 1; i < len(data) && i < 10; i++ {
        b := *(*byte)(unsafe.Add(unsafe.Pointer(&data[0]), i))
        size |= uint64(b&0x7f) << shift
        shift += 7
        if b&0x80 == 0 {
            return objType, size, i + 1
        }
    }

    return ObjBad, 0, -1
}
```

### Object Decompression

Once we have the type and size, we can choose the optimal decompression strategy:

```go
// From the actual readRawObject implementation
func readRawObject(r *mmap.ReaderAt, off uint64) (ObjectType, []byte, error) {
    // Read first 32 bytes for header (usually enough)
    var headerBuf [32]byte
    n, err := r.ReadAt(headerBuf[:], int64(off))
    if err != nil && err != io.EOF {
        return ObjBad, nil, err
    }

    hdr := headerBuf[:n]
    objType, size, headerLen := parseObjectHeaderUnsafe(hdr)
    if headerLen <= 0 || headerLen > n {
        // Handle cases where header is > 32 bytes
        // ... extended header reading logic ...
    }

    // For small objects, read everything at once
    const (
        smallObjectThreshold = 65536 // 64KB threshold
        zlibOverheadSize     = 1024  // Extra space for zlib compression overhead
    )

    if size < smallObjectThreshold {
        compressedBuf := make([]byte, size+zlibOverheadSize) // Extra space for zlib overhead
        n, err := r.ReadAt(compressedBuf, int64(off)+int64(headerLen))
        if err != nil && err != io.EOF {
            return ObjBad, nil, err
        }

        zr, err := zlib.NewReader(bytes.NewReader(compressedBuf[:n]))
        if err != nil {
            return ObjBad, nil, err
        }
        defer zr.Close()

        result := make([]byte, 0, size)
        buf := bytes.NewBuffer(result)
        _, err = io.Copy(buf, zr)
        return objType, buf.Bytes(), nil
    }

    // For large objects, use streaming.
    compressedLen := int64(size) + zlibOverheadSize
    if compressedLen <= 0 {
        return ObjBad, nil, errors.New("invalid compressed length")
    }
    src := io.NewSectionReader(r, int64(off)+int64(headerLen), compressedLen)
    zr, err := zlib.NewReader(src)
    if err != nil {
        return ObjBad, nil, err
    }
    defer zr.Close()

    var buf bytes.Buffer
    buf.Grow(int(size)) // Pre-allocate
    if _, err = buf.ReadFrom(zr); err != nil {
        return ObjBad, nil, err
    }

    return objType, buf.Bytes(), nil
}
```

If the object's type is a standard commit, tree, or blob, our work is done. But most of the time, we find a delta.

---

## âš¡ The Real Magic: Delta Compression

To save enormous amounts of space, Git stores most objects as **deltas** - a set of instructions for transforming a "base" object into the target object.

**Like a GPS navigation system** - instead of storing millions of complete route maps for every possible journey, it stores one base map and generates turn-by-turn instructions ('turn left in 500ft, continue straight for 2m') to transform any starting point into any destination. Git deltas work the same way, storing just the differences between file versions.

When you have 500 commits changing one line in a 10,000-line file, Git stores the original once plus 499 tiny instruction sets saying "change line 42." Your `.git` folder stays small instead of exploding to gigabytes.

### Two Types of Deltas

**OFS_DELTA (Offset Delta)**
- Base object is in the same packfile
- Referenced by negative offset from current position
- Most common type

**REF_DELTA (Reference Delta)**
- Base object referenced by full 20-byte SHA-1 hash
- Base may be in different packfile

### Delta Header Parsing

```go
// From the actual parseDeltaHeader implementation
func parseDeltaHeader(t ObjectType, data []byte) (Hash, uint64, []byte, error) {
    var h Hash

    if t == ObjRefDelta {
        if len(data) < 20 {
            return h, 0, nil, fmt.Errorf("ref delta too short")
        }
        *(*Hash)(unsafe.Pointer(&h[0])) = *(*Hash)(unsafe.Pointer(&data[0]))
        return h, 0, data[20:], nil
    }

    // Ofs-delta with optimized varint parsing
    if len(data) == 0 {
        return h, 0, nil, fmt.Errorf("ofs delta too short")
    }

    b0 := *(*byte)(unsafe.Pointer(&data[0]))
    off := uint64(b0 & 0x7f)

    if b0&0x80 == 0 {
        return h, off, data[1:], nil
    }

    // Unrolled loop for common cases
    i := 1
    for i < len(data) && i < 10 {
        b := *(*byte)(unsafe.Add(unsafe.Pointer(&data[0]), i))
        off = (off + 1) << 7
        off |= uint64(b & 0x7f)
        i++
        if b&0x80 == 0 {
            break
        }
    }

    if i >= len(data) {
        return h, 0, nil, fmt.Errorf("invalid ofs delta encoding")
    }

    return h, off, data[i:], nil
}
```

### Delta Application

Delta data contains a stream of two opcodes:

**Copy Opcode (MSB=1)**: Copy bytes from base object to output
**Insert Opcode (MSB=0)**: Insert literal bytes from delta stream

**Like following GPS turn-by-turn directions** - The Copy opcode is like "Continue on this road for 500 feet" (copying from the base map), and the Insert opcode is like "Turn left onto 'main.go' street" (inserting a new instruction). A delta is just a stream of these commands that navigate from the base version to the target version.

```go
// From the actual applyDelta implementation
func applyDelta(base, delta []byte) []byte {
    if len(delta) == 0 {
        return nil
    }

    _, n1 := decodeVarInt(delta)
    if n1 <= 0 || n1 >= len(delta) {
        return nil
    }
    targetSize, n2 := decodeVarInt(delta[n1:])
    if n2 <= 0 || n1+n2 >= len(delta) {
        return nil
    }

    out := make([]byte, targetSize)

    deltaLen := len(delta)
    baseLen := len(base)
    opIdx := n1 + n2
    outIdx := 0

    for opIdx < deltaLen {
        op := *(*byte)(unsafe.Add(unsafe.Pointer(&delta[0]), opIdx))
        opIdx++

        if op&0x80 != 0 { // Copy operation
            var cpOff, cpLen uint32

            // Unrolled offset parsing
            if op&0x01 != 0 {
                if opIdx >= deltaLen {
                    return nil
                }
                cpOff = uint32(*(*byte)(unsafe.Add(unsafe.Pointer(&delta[0]), opIdx)))
                opIdx++
            }
            if op&0x02 != 0 {
                if opIdx >= deltaLen {
                    return nil
                }
                cpOff |= uint32(*(*byte)(unsafe.Add(unsafe.Pointer(&delta[0]), opIdx))) << 8
                opIdx++
            }
            // ... similar for remaining offset bytes ...

            // Unrolled length parsing
            if op&0x10 != 0 {
                if opIdx >= deltaLen {
                    return nil
                }
                cpLen = uint32(*(*byte)(unsafe.Add(unsafe.Pointer(&delta[0]), opIdx)))
                opIdx++
            }
            // ... similar for remaining length bytes ...

            if cpLen == 0 {
                cpLen = 65536
            }

            if int(cpOff)+int(cpLen) > baseLen || outIdx+int(cpLen) > int(targetSize) {
                return nil
            }

            // Use memmove for potentially overlapping memory
            copyMemory(
                unsafe.Add(unsafe.Pointer(&out[0]), outIdx),
                unsafe.Add(unsafe.Pointer(&base[0]), cpOff),
                int(cpLen),
            )
            outIdx += int(cpLen)

        } else if op != 0 { // Insert operation
            insertLen := int(op)
            if opIdx+insertLen > deltaLen || outIdx+insertLen > int(targetSize) {
                return nil
            }

            // Direct memory copy
            copyMemory(
                unsafe.Add(unsafe.Pointer(&out[0]), outIdx),
                unsafe.Add(unsafe.Pointer(&delta[0]), opIdx),
                insertLen,
            )
            opIdx += insertLen
            outIdx += insertLen
        } else {
            return nil // Invalid op
        }
    }

    return out
}
```

---

## ðŸŽ¯ Preventing Re-inflation: A Git-Optimized Cache

Delta resolution creates a unique caching challenge: objects are needed multiple times during long-running operations, but traditional LRU caches can't distinguish between "recently accessed" and "actively being used."

### An Analogy: A Library Checkout System

To understand the problem and the solution, think of a library managing its most popular books.

-   **A standard LRU cache is like a library with an overzealous, automated return policy.** It sees a textbook sitting on a table, notices it hasn't been physically touched in an hour, and whisks it back to deep storage. But a student was just about to use that textbook for their open-book exam! Now they have to waste precious time requesting it again. The system's focus on "recent use" completely missed that the book was still "in use."

-   **Our reference-counted cache is like a proper checkout system.** When the student checks out the textbook, they get a handle for it. The book cannot be returned to storage until that handle is explicitly returned. Even if the book sits on the table for hours, it's considered "in use" and remains available because the checkout is still active. Multiple students can check out the same book, and it will only be eligible for storage once every single one of them has returned it. This is exactly how our `refCountedDeltaWindow` works: an object cannot be evicted while a `Handle` to it exists.

### The Re-inflation Problem

Consider this scenario during delta chain resolution:

```go
// Thread 1: Resolving a deep delta chain (takes 100ms total)
base := cache.Get("large-base-1MB")     // Expensive: 5ms to inflate
delta1 := applyDelta(base, getDelta1())   // 20ms
delta2 := applyDelta(delta1, getDelta2()) // 20ms
// ... doing more work, but base is still "logically in use"

// Meanwhile, Thread 2-8: Processing other objects
// They fill the cache with their work, evicting "large-base-1MB"

// Thread 1: Needs the base again for final step
base2 := cache.Get("large-base-1MB")    // CACHE MISS!
// Must re-read from pack: 2ms disk I/O
// Must re-inflate: 5ms decompression
// 7ms penalty for something we already had!
```

For large packfiles with deep delta chains and high concurrency, this re-inflation cost becomes significant. Large base objects (256KB-1MB+) are expensive to decompress, and delta chains often need the same base object multiple times.

### Reference Counting Solution

Instead of a simple LRU cache, we use reference counting to track which objects are actively being used:

```go
// With reference counting:
base := cache.acquire("large-base")       // Inflates once: 5ms
// base now has refCnt=1, cannot be evicted!

delta1 := applyDelta(base.Data(), getDelta1()) // 20ms
delta2 := applyDelta(base.Data(), getDelta2()) // 20ms
// ... more work, base stays in cache

finalResult := applyDelta(base.Data(), getFinalDelta()) // Uses cached copy!
base.Release() // Now eligible for eviction
```

The key insight: objects with active references cannot be evicted, preventing expensive re-inflation during multi-step operations. This cache design is specifically tailored to Git's delta resolution patterns where objects need to be held across multiple operations.

---

## ðŸ›¡ï¸ Staying Safe: Cycle and Depth Protection

A malformed packfile could contain delta chains that reference themselves or are pathologically deep. We use a `deltaContext` to guard against this:

```go
// From the actual deltaContext implementation
type deltaContext struct {
    // visited records every base object reached by object ID during the
    // current resolution. It lets the resolver detect ref-delta cycles.
    visited map[Hash]bool

    // offsets records every packfile offset reached during ofs-delta
    // resolution. It lets the resolver detect cycles that reference a
    // previously visited object in the same pack.
    offsets map[uint64]bool

    // depth is the current recursion depth. It is incremented on entry to
    // a child delta and decremented on exit.
### An Analogy: A Library Checkout System

To understand the problem and the solution, think of a library managing its most popular books.

-   **A standard LRU cache is like a library with an overzealous, automated return policy.** It sees a textbook sitting on a table, notices it hasn't been physically touched in an hour, and whisks it back to deep storage. But a student was just about to use that textbook for their open-book exam! Now they have to waste precious time requesting it again. The system's focus on "recent use" completely missed that the book was still "in use."

-   **Our reference-counted cache is like a proper checkout system.** When the student checks out the textbook, they get a handle for it. The book cannot be returned to storage until that handle is explicitly returned. Even if the book sits on the table for hours, it's considered "in use" and remains available because the checkout is still active. Multiple students can check out the same book, and it will only be eligible for storage once every single one of them has returned it. This is exactly how our `refCountedDeltaWindow` works: an object cannot be evicted while a `Handle` to it exists.

    depth int

    // maxDepth is the maximum permitted depth before resolution aborts
    maxDepth int
}

func (ctx *deltaContext) checkRefDelta(hash Hash) error {
    // Guard against unbounded recursion
    if ctx.depth >= ctx.maxDepth {
        return fmt.Errorf("delta chain too deep (max %d)", ctx.maxDepth)
    }
    // Detect ref-delta cycles
    if ctx.visited[hash] {
        return fmt.Errorf("circular delta reference detected for %x", hash)
    }
    return nil
}

func (ctx *deltaContext) enterRefDelta(hash Hash) {
    ctx.visited[hash] = true
    ctx.depth++
}

func (ctx *deltaContext) exit() {
    ctx.depth--
}
```

This makes our resolver robust against corrupted or malicious repository data.

---

## ðŸš€ The Complete Pipeline

Here's how all the pieces work together in the actual Store implementation:

```go
// From the actual Store.Get implementation
func (s *Store) Get(oid Hash) ([]byte, ObjectType, error) {
    // Fast path: check the reference-counted delta window
    if handle, ok := s.dw.acquire(oid); ok {
        defer handle.Release()
        return handle.Data(), detectType(handle.Data()), nil
    }

    // Second-level lookup in the larger ARC cache
    if b, ok := s.cache.Get(oid); ok {
        return b, detectType(b), nil
    }

    // Cache miss â€“ inflate from pack with delta context
    return s.getWithContext(oid, newDeltaContext(s.maxDeltaDepth))
}

func (s *Store) inflateFromPack(
    p *mmap.ReaderAt,
    off uint64,
    oid Hash,
    ctx *deltaContext,
) ([]byte, ObjectType, error) {
    // Quick header peek â€“ avoids decompression for commit objects
    objType, _, err := peekObjectType(p, off)
    if err != nil {
        return nil, ObjBad, err
    }

    if objType == ObjCommit {
        // Commit-graph supplies metadata â†’ skip inflation entirely
        return nil, ObjCommit, nil
    }

    // For non-commits, we need the full object data
    _, data, err := readRawObject(p, off)
    if err != nil {
        return nil, ObjBad, err
    }

    switch objType {
    case ObjBlob, ObjTree, ObjTag:
        // CRC verification if enabled
        if s.VerifyCRC {
            crc, found := s.findCRCForObject(p, off, oid)
            if found {
                if err := s.verifyCRCForPackObject(p, off, crc); err != nil {
                    return nil, ObjBad, err
                }
            }
        }
        s.dw.add(oid, data)        // Add to delta window
        s.cache.Add(oid, data)     // Add to main cache
        return data, objType, nil

    case ObjOfsDelta, ObjRefDelta:
        baseHash, baseOff, deltaBuf, err := parseDeltaHeader(objType, data)
        if err != nil {
            return nil, ObjBad, err
        }

        var base []byte
        if objType == ObjRefDelta {
            if err := ctx.checkRefDelta(baseHash); err != nil {
                return nil, ObjBad, err
            }
            ctx.enterRefDelta(baseHash)
            base, _, err = s.getWithContext(baseHash, ctx)
            ctx.exit()
        } else {
            if err := ctx.checkOfsDelta(baseOff); err != nil {
                return nil, ObjBad, err
            }
            ctx.enterOfsDelta(baseOff)
            base, _, err = readObjectAtOffsetWithContext(p, baseOff, s, ctx)
            ctx.exit()
        }
        if err != nil {
            return nil, ObjBad, err
        }

        full := applyDelta(base, deltaBuf)
        s.dw.add(oid, full)        // Cache for future delta resolution
        s.cache.Add(oid, full)     // Cache for general access
        return full, detectType(full), nil

    default:
        return nil, ObjBad, fmt.Errorf("unknown obj type %d", objType)
    }
}
```

---

## ðŸŽ¯ Key Takeaways

We've conquered the packfile with a complete implementation:

**Header Parsing**: Efficiently extract object type and size from variable-length headers using unsafe operations for maximum performance.

**Zlib Decompression**: Smart handling of small vs large objects to optimize memory usage and decompression speed.

**Delta Resolution**: Reconstruct objects from delta chains with robust cycle detection and depth limits.

**Git-Optimized Caching**: Reference-counted cache designed specifically for Git's delta resolution patterns, delivering 10-17x performance improvements over generic LRU for Git workloads.

**Concurrency Safety**: Handle-based API prevents data races and use-after-free bugs in multi-threaded environments.

**Performance**: Two-level caching strategy optimized for Git's access patterns, with memory-mapped I/O and unsafe optimizations.

This implementation handles all Git object types and storage formats, providing a complete foundation for accessing repository data safely and efficiently.

---

## ðŸ”® Up Next

We now have the complete object retrieval pipeline. We can take any SHA-1 hash, find it in a packfile, decompress it, and safely resolve any delta chains to get fully-inflated object data.

However, to walk the repository's history efficiently, we need to avoid decompressing every single commit object just to find its parents. This is where Git's secret weapon comes in.

In **Part 5: The Commit-Graph Advantage**, we'll explore:

- How the commit-graph file provides a super-fast lane for history traversal.
- Parsing the binary format of the commit-graph.
- Building a component that reads this file to find commit parents and root trees without ever touching the packfile.

This is the key to unlocking high-speed history walks, laying the foundation for our efficient diff engine.
