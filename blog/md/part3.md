# ðŸ”“ Under the Hood: Unpacking Git's Secrets
**Part 3: The Multi-Pack Index â€“ Git's Ultimate Performance Hack**

Welcome back to our deep dive into Git's internals! In [Part 2](./part2.md), we built a blazing-fast pack index parser that could find any object in a single packfile. But what happens when your repository accumulates dozens or even hundreds of packfiles? Today, we're tackling Git's elegant solution: the multi-pack index (`.midx`).

But first, let me share a story that perfectly illustrates why this technology is so brilliant.

---

## ðŸª The Warehouse Problem

**Imagine you're managing a massive warehouse with thousands of productsâ€¦**

**Without Multi-Pack Index (âŒ):**
Products scattered across 50 storage units:

```
Unit 1: [Products A-C]
Unit 2: [Products B-D]
Unit 3: [Products C-E]
...
Unit 50: [Products X-Z]
```

**Finding Product "M":**
> Check Unit 1 index... not there
> Check Unit 2 index... not there
> Check Unit 3 index... not there
> ... (potentially checking all 50 units!)

**With Multi-Pack Index (âœ…):**
Master inventory system + individual unit manifests:

```
Master Index:
- Product M â†’ Unit 27, Shelf 3
- Product N â†’ Unit 12, Shelf 8
- ...
```

**Finding Product "M":**
1. Check master index â†’ "Unit 27, Shelf 3"
2. Go directly to Unit 27
3. Retrieved!

> **This is exactly how Git's multi-pack index works!** Instead of searching through every packfile's index, we have one master index that tells us exactly which pack contains each object.

---

## Why Multiple Packfiles Exist

### The Natural Evolution of a Repository

As your repository grows, Git doesn't keep repacking everything into one massive packfile. Here's why:

1. **Incremental Updates**: Each `git fetch` or `git push` can create new packfiles
2. **Performance**: Repacking large repositories is expensive
3. **Concurrency**: Multiple operations can create packs simultaneously
4. **Historical Accumulation**: Years of development = many packfiles

```bash
# A real-world example from a large repository
$ ls .git/objects/pack/*.pack | wc -l
127  # 127 packfiles!

# Without midx: potentially 127 binary searches to find one object!
# With midx: just 1 binary search!
```

---

## ðŸ—ï¸ The Multi-Pack Index Format

### Multi-Pack Index File Layout

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Header (12 bytes)      â”‚ â†’ Magic, version, metadata
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Chunk Table            â”‚ â†’ Directory of data chunks
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  PNAM Chunk             â”‚ â†’ Pack names (which packs we index)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  OIDF Chunk             â”‚ â†’ Fanout table (our old friend!)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  OIDL Chunk             â”‚ â†’ Sorted object IDs
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  OOFF Chunk             â”‚ â†’ Pack ID + offset pairs
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  LOFF Chunk (optional)  â”‚ â†’ Large offsets (>2GB)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Let's parse this step by step!

---

## Parsing the Multi-Pack Index

### Step 1: Read and Validate the Header

```go
// Multi-pack index chunk identifiers.
const (
    chunkPNAM = 0x504e414d // 'PNAM' - pack names
    chunkOIDF = 0x4f494446 // 'OIDF' - object ID fanout table
    chunkOIDL = 0x4f49444c // 'OIDL' - object ID list
    chunkOOFF = 0x4f4f4646 // 'OOFF' - object offsets
    chunkLOFF = 0x4c4f4646 // 'LOFF' - large object offsets
)

func parseMidx(dir string, mr *mmap.ReaderAt) (*midxFile, error) {
    // Read the 12-byte header
    var hdr [12]byte
    if _, err := mr.ReadAt(hdr[:], 0); err != nil {
        return nil, err
    }

    // Verify magic signature: "MIDX"
    if !bytes.Equal(hdr[0:4], []byte("MIDX")) {
        return nil, fmt.Errorf("not a MIDX file")
    }

    // Check version (we support version 1)
    if hdr[4] != 1 {
        return nil, fmt.Errorf("unsupported midx version %d", hdr[4])
    }

    // Hash algorithm (1 = SHA-1)
    if hdr[5] != 1 {
        return nil, fmt.Errorf("only SHA-1 midx supported")
    }

    // Extract metadata
    chunks := int(hdr[6])                                    // Number of chunks
    packCount := int(binary.BigEndian.Uint32(hdr[8:12]))   // Number of packs

    fmt.Printf("MIDX: %d chunks, indexing %d packs\n", chunks, packCount)
    // ...
}
```

> **Header Breakdown:**
> - **Bytes 0-3**: Magic "MIDX" (0x4D494458)
> - **Byte 4**: Version number
> - **Byte 5**: Hash algorithm ID
> - **Byte 6**: Number of chunks
> - **Byte 7**: Reserved (must be 0)
> - **Bytes 8-11**: Number of base packfiles

### Step 2: Parse the Chunk Table

The chunk table is like a table of contents, telling us where each data section lives:

```go
// Chunk table entry structure
type chunkDesc struct {
    id  [4]byte  // 4-character chunk ID
    off uint64   // Offset where this chunk starts
}

// Read all chunk entries
cd := make([]chunkDesc, chunks+1) // +1 for sentinel
for i := 0; i < len(cd); i++ {
    var row [12]byte
    base := int64(12 + i*12)  // After header
    if _, err := mr.ReadAt(row[:], base); err != nil {
        return nil, err
    }

    copy(cd[i].id[:], row[0:4])
    cd[i].off = binary.BigEndian.Uint64(row[4:12])
}

// Sort by offset to calculate sizes
sort.Slice(cd, func(i, j int) bool {
    return cd[i].off < cd[j].off
})

findChunk := func(id uint32) (off int64, size int64, err error) {
    for i := 0; i < len(cd)-1; i++ {
        chunkID := binary.BigEndian.Uint32(cd[i].id[:])
        if chunkID == id {
            return int64(cd[i].off),
                int64(cd[i+1].off) - int64(cd[i].off),
                nil
        }
    }
    return 0, 0, fmt.Errorf("chunk %08x not found", id)
}
```

### Step 3: Parse PNAM (Pack Names)

This chunk lists all the packfiles that the midx indexes:

```go
// Find and read the PNAM chunk
pnOff, pnSize, err := findChunk(chunkPNAM)
if err != nil {
    return nil, err
}

pnamData := make([]byte, pnSize)
if _, err = mr.ReadAt(pnamData, pnOff); err != nil {
    return nil, err
}

// Pack names are NUL-terminated strings
var packNames []string
for start := 0; start < len(pnamData); {
    end := bytes.IndexByte(pnamData[start:], 0)
    if end <= 0 {
        break
    }
    packNames = append(packNames, string(pnamData[start:start+end]))
    start += end + 1
}

// Example output:
// pack-a7f3b2c1d9e5f0a8b4c6d2e7f3a9b5c1d7e3f9a0.pack
// pack-b8e4c3d2e0f6a1b9c5d7e3f8a4b0c6d2e8f4a0b1.pack
// pack-c9f5d4e3f1a7b2c0d6e8f4a9b5c1d3e9f5a1b2c3.pack
```

> **Why track pack names?**
> When we find an object, we need to know which physical `.pack` file to read from!

### Step 4: Parse OIDF (Fanout Table)

Our old friend returns! Just like in `.idx` files:

```go
// Read the fanout table
fanOff, _, err := findChunk(chunkOIDF)
if err != nil {
    return nil, err
}

var fanout [256]uint32
fanoutBytes := unsafe.Slice((*byte)(unsafe.Pointer(&fanout[0])), 1024)
if _, err = mr.ReadAt(fanoutBytes, fanOff); err != nil {
    return nil, err
}

// Convert from big-endian if needed
if isLittleEndian() {
    for i := range fanout {
        fanout[i] = bswap32(fanout[i])
    }
}

totalObjects := fanout[255]
fmt.Printf("MIDX indexes %d total objects\n", totalObjects)
```

### Step 5: Parse OIDL (Object ID List)

All object hashes, sorted for binary search:

```go
// Read all object IDs
oidOff, _, err := findChunk(chunkOIDL)
if err != nil {
    return nil, err
}

objectIDs := make([]Hash, totalObjects)
oidBytes := unsafe.Slice((*byte)(unsafe.Pointer(&objectIDs[0])),
                        int(totalObjects*20))
if _, err = mr.ReadAt(oidBytes, oidOff); err != nil {
    return nil, err
}
```

### Step 6: Parse OOFF (Object Offsets)

This is where the magic happens - mapping objects to their pack + offset:

```go
// Read object offset data
offOff, _, err := findChunk(chunkOOFF)
if err != nil {
    return nil, err
}

// Each entry is 8 bytes: 4 for pack ID, 4 for offset
offsetData := make([]byte, totalObjects*8)
if _, err = mr.ReadAt(offsetData, offOff); err != nil {
    return nil, err
}

// Parse entries
entries := make([]struct {
    packID uint32
    offset uint64
}, totalObjects)

for i := uint32(0); i < totalObjects; i++ {
    base := i * 8
    packID := binary.BigEndian.Uint32(offsetData[base:base+4])
    rawOffset := binary.BigEndian.Uint32(offsetData[base+4:base+8])

    // Handle large offsets (MSB set = index into LOFF chunk)
    var offset uint64
    if rawOffset&0x80000000 == 0 {
        offset = uint64(rawOffset)
    } else {
        // Large offset - we'll handle this next!
        largeIdx := rawOffset & 0x7FFFFFFF
        offset = largeOffsets[largeIdx]  // From LOFF chunk
    }

    entries[i] = struct {
        packID uint32
        offset uint64
    }{packID, offset}
}
```

### Step 7: Handle Large Offsets (LOFF)

For packfiles larger than 2GB, we need the optional LOFF chunk:

```go
// LOFF is optional - only present if any pack exceeds 2GB
loffOff, loffSize, _ := findChunk(chunkLOFF) // ignore "not found" error
var largeOffsets []uint64
if loffSize > 0 {
    largeOffsets = make([]uint64, loffSize/8)
    loffBytes := unsafe.Slice((*byte)(unsafe.Pointer(&largeOffsets[0])),
                              int(loffSize))
    if _, err = mr.ReadAt(loffBytes, loffOff); err != nil {
        return nil, err
    }

    // Convert from big-endian
    if isLittleEndian() {
        for i := range largeOffsets {
            largeOffsets[i] = binary.BigEndian.Uint64(
                (*[8]byte)(unsafe.Pointer(&largeOffsets[i]))[:])
        }
    }
}

// Now we can resolve large offset references in the OOFF parsing above
```

---

## Pack Cache Optimization

Here's a useful optimization for our implementation. When parsing a multi-pack index, the same packfile might be referenced by both the midx AND individual idx files. Without caching, we'd mmap the same file twice!

```go
// In Open(), we maintain a pack cache
packCache := make(map[string]*mmap.ReaderAt)

// When the midx parser needs a pack...
func parseMidx(dir string, mr *mmap.ReaderAt, packCache map[string]*mmap.ReaderAt) (*midxFile, error) {
    // ... parse pack names ...

    for i, name := range packNames {
        p := filepath.Join(dir, name)
        if h, ok := packCache[p]; ok {
            packs[i] = h  // Reuse existing mmap!
            continue
        }
        // Only mmap if we haven't seen this pack before
        r, err := mmap.Open(p)
        packCache[p] = r
    }
}
```

By sharing mmap handles between midx and idx parsing, we avoid duplicate memory mappings. On a repository with 100 packs, this saves 100 system calls and prevents confusing the OS's page cache.

### Real-World Impact
```bash
# Without pack cache: Each subsystem mmaps independently
$ strace -e openat git cat-file -p HEAD~1000:Makefile 2>&1 | grep pack | wc -l
247  # Opening same packs multiple times!

# With pack cache: Shared mmap handles
3    # Each pack opened exactly once!
```

#### Breaking Down the Numbers

Let's trace through what happens when Git resolves `HEAD~1000:Makefile` to understand this dramatic improvement:

**Without Pack Cache (247 opens):**
```bash
# Git needs to find 3 objects: commit â†’ tree â†’ blob
# Each subsystem searches independently through all packs

# 1. Midx parser searches for commit object
#    - Potentially checks pack-001.pack, pack-002.pack, ..., pack-127.pack
#    - Finds commit in pack-045.pack

# 2. Individual idx parser searches for tree object
#    - Opens pack-001.pack, pack-002.pack, ..., pack-127.pack AGAIN
#    - Finds tree in pack-078.pack

# 3. Another subsystem searches for blob object
#    - Opens pack-001.pack, pack-002.pack, ..., pack-127.pack AGAIN
#    - Finds blob in pack-012.pack

# Total: ~127 packs Ã— ~2-3 subsystems â‰ˆ 247 open() calls
```

**With Pack Cache (3 opens):**
```bash
# Midx tells us exactly which pack contains each object
# Pack handles are shared across all subsystems

# 1. First lookup (commit object)
#    - Cache miss: Opens pack-045.pack
#    - Finds commit, stores pack-045.pack handle in cache

# 2. Second lookup (tree object)
#    - Cache miss: Opens pack-078.pack
#    - Finds tree, stores pack-078.pack handle in cache

# 3. Third lookup (blob object)
#    - Cache miss: Opens pack-012.pack
#    - Finds blob, stores pack-012.pack handle in cache

# Total: Only 3 open() calls for the 3 packs that actually contain our objects
```

The key insight: **the midx eliminates the search**, and **the cache eliminates redundant opens**. Instead of every subsystem independently searching through potentially all 127 packs, we make exactly one targeted open per pack that actually contains our data.

---

## The Complete Lookup Process

Here's how it all comes together:

```go
func (m *midxFile) findObject(hash Hash) (*mmap.ReaderAt, uint64, bool) {
    // Step 1: Use fanout to narrow search range
    firstByte := hash[0]
    start := uint32(0)
    if firstByte > 0 {
        start = m.fanout[firstByte-1]
    }
    end := m.fanout[firstByte]

    // Step 2: Binary search within range
    relIdx, found := slices.BinarySearchFunc(
        m.objectIDs[start:end],
        hash,
        func(a, b Hash) int { return bytes.Compare(a[:], b[:]) },
    )
    if !found {
        return nil, 0, false
    }

    // Step 3: Get pack and offset
    absIdx := int(start) + relIdx
    entry := m.entries[absIdx]

    // Step 4: Return the right pack's mmap handle
    return m.packReaders[entry.packID], entry.offset, true
}
```

---

## ðŸš€ Performance Analysis

Let's see the dramatic performance improvement:

### Without Multi-Pack Index
```go
// Linear search through all packs
for _, pack := range store.packs {
    if offset, found := pack.findObject(hash); found {
        return pack, offset, true
    }
}
// Worst case: N pack lookups, each with log(M) binary search
// Total: O(N Ã— log M)
```

### With Multi-Pack Index
```go
// Single lookup in master index
if pack, offset, found := store.midx.findObject(hash); found {
    return pack, offset, true
}
// Always: O(log T) where T is total objects across all packs
```

### Real-World Impact

```bash
# Benchmark on Linux kernel repository (127 packfiles)
$ time git cat-file -p HEAD~10000:Makefile

# Without midx: 0.085s (searches ~50 packs on average)
# With midx: 0.012s (single index lookup)
# 7Ã— faster!
```

---

## Testing Our Implementation

Let's verify our midx parser works correctly:

```go
func TestMultiPackIndex(t *testing.T) {
    // Create a test repository with multiple packs
    dir := t.TempDir()

    // Open the store (it will auto-detect midx)
    store, err := Open(filepath.Join(dir, ".git/objects/pack"))
    require.NoError(t, err)
    defer store.Close()

    // Verify midx was loaded
    assert.NotNil(t, store.midx)
    fmt.Printf("MIDX loaded: %d packs, %d objects\n",
               len(store.midx.packNames),
               len(store.midx.objectIDs))

    // Test object lookup
    someHash := store.midx.objectIDs[0]
    data, objType, err := store.Get(someHash)
    require.NoError(t, err)

    fmt.Printf("Retrieved %s object (%d bytes) via midx\n",
               objType, len(data))
}
```

---

## Key Insights

### The Brilliance of the Design

1. **Single Index**: One binary search instead of N
2. **Memory Efficiency**: Deduplicates objects that appear in multiple packs
3. **Incremental**: Can be regenerated as packs are added/removed
4. **Backwards Compatible**: Git falls back to individual indexes if midx is missing

### When Git Creates a Multi-Pack Index

```bash
# Manually create/update midx
$ git multi-pack-index write

# Automatic during maintenance
$ git maintenance run --task=incremental-repack

# Verify midx contents
$ git multi-pack-index verify
```

---

## ðŸ”® Coming Up Next

We've now built the complete foundation for reading Git's object database:
- âœ… Pack index parsing (Part 2)
- âœ… Multi-pack index support (Part 3)
- ðŸ”œ **Part 4**: Reading and decompressing actual pack data
- ðŸ”œ **Part 5**: Resolving delta chains
- ðŸ”œ **Part 6**: Walking commits and generating diffs

The multi-pack index was our last piece of "finding" objects efficiently. Next, we'll dive into actually reading and decompressing the object data from packfiles. Get ready to explore zlib compression, delta chains, and Git's clever storage optimizations!

> **Building Block Complete!**
> With midx support, our object store can now handle repositories of any size efficiently. Whether you have 1 packfile or 1000, object lookups remain lightning fast!

---

### References

- [Git Documentation - Multi-Pack-Index Format](https://git-scm.com/docs/gitformat-pack#_multi_pack_index_midx_files)
- [Git Source: midx.c](https://github.com/git/git/blob/master/midx.c)
- [Derrick Stolee's Blog - Scaling Git's Garbage Collection](https://devblogs.microsoft.com/devops/scaling-gits-garbage-collection/)
